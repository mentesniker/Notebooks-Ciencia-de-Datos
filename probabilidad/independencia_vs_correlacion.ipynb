{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "independencia_vs_correlacion.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LdchVv-l03"
      },
      "source": [
        "## Independencia probabilística (independencia de eventos)\n",
        "\n",
        "Sea $E$ un experimento y $\\Omega$ el espacio muestral correspondiente. Consideremos $A$ y $B$ eventos disjuntos en $\\Omega$, decimos que $A$ es **independiente** de $B$, denotado como $A\\perp B$, si se cumple la siguiente igualdad:\n",
        "\n",
        "$$P(A|B)={P(A\\cap B)\\over P(B)}= P(A)$$ \n",
        "\n",
        "siempre que \n",
        "\n",
        "$$B \\neq \\phi~,~P(B)\\neq 0$$\n",
        "\n",
        "y esta implica que\n",
        "\n",
        "$$P(B|A)=P(B)$$\n",
        "\n",
        "y por lo tanto se requiere que \n",
        "\n",
        "$$A \\neq \\phi~,~P(A)\\neq 0$$\n",
        "\n",
        "De lo anterior se puede concluir que si $A$ es independiente de $B$ la regla de la multiplicación se transforma en:\n",
        "\n",
        "$$P(A\\cap B) = P(A|B)P(B) = P(A)P(B|A) = P(A)P(B)$$\n",
        "\n",
        "Vale la pena considerar el caso especial en el que $A\\cap B = \\phi$, es decir $A$ y $B$ son **eventos mutuamente excluyentes**, tenemos que:\n",
        "\n",
        "$$P(A\\cap B) = 0 $$\n",
        "\n",
        "y entonces podemos concluir que:\n",
        "\n",
        "$$P(A|B) = {0 \\over P(B)} = 0 = {0 \\over P(A)} = P(B|A)$$\n",
        "\n",
        "Este concepto es uno de los más importantes en probabilidad y estadística, ya que varios de los modelos o algoritmos que se les desea aplicar a un conjunto de datos presuponen que éstos están conformados por observaciones o unidades estadísticas independientes entre sí, es decir que la ocurrencia o el valor que toma alguna observación no condiciona la ocurrencia de las demás.\n",
        "\n",
        "**Ejercicio:**\n",
        "\n",
        "Sean $A$, $B$ y $C$ eventos, subconjuntos de $\\Omega$, considera:\n",
        "\n",
        "1. $A\\cap B \\neq \\phi$\n",
        "2. $I_1 = A\\cap C \\neq \\phi$\n",
        "3. $I_2 = B\\cap C \\neq \\phi$\n",
        "4. $I_3 = I_1 \\cap I_2 = \\phi$\n",
        "\n",
        "Si A es Independiente de B, es decir $P(A|B)=P(A)$ y $P(B|A)=P(B)$, por lo tanto $P(A\\cap B)=P(A)P(B)$, qué puedes decir del evento A|C respecto al evetno B|C, ¿la independencia entre A y B se preserva? (Pista: utiliza un diagrama de Venn para orientar tu respuesta).\n",
        "\n",
        "## Independencia de eventos vs Independencia de observaciones\n",
        "\n",
        "Nosotros acabamos de definir eventos independientes, recordemos que los eventos son subconjuntos de $\\Omega$, mientras que las observaciones son elementos que pertenencen a alguno de estos subconjuntos. Consideremos el caso extremo en el que cada observación $o_i$ es el único elemento del evento $E_i$, entonces tendremos tantos eventos como observaciones y la definición de independencia puede aplicársele a esta colección de observaciones que constituyen al conjunto de datos.\n",
        "\n",
        "Esta condición puede ser verificada mediante diversas pruebas que pueden aplicársele al dataset, sin embargo, es un aspecto que debe de controlarse desde el diseño del experimento y de antemano saberse que \"la independencia absoluta\" es un concepto idealizado que muchas veces suele ser reemplazado por el de \"dependencia débil\", y a su vez este de dependencia por el de autocorrelación entre predictores o variables independientes. \n",
        "\n",
        "Más adelante definiremos con precisión el término **correlación**. De momento consideremos que aunque las palabras: **dependencia/independencia**, **correlación/no correlación** y **oblicuidad/ortogonalidad** pese a no ser sinónimas presentan equivalencias interesantes entre sí. Por ejemplo: si dos variables son independientes su coeficiente de correlación de Pearson debe de ser cero, si dos variables son dependientes este coeficiente es distinto de cero, sin embargo si el coeficiente de correlación es cero no podemos decir que dos variables sean independientes (¿por qué? busca un ejemplo en internet).\n",
        "\n",
        "Una manera de poner a prueba si nuestro conjunto de datos cuenta con observaciones independientes (y en el caso de una regresión lineal: con independencia entre los residuales asociados a cada observación) es mediante la **prueba de Durbin-Watson**. En la cual no se prueba independencia como tal entre variables sino la presencia de autocorrelación entre las mismas, en dicha prueba si se obtiene un valor menor a 2 para el estadístico de prueba se dice que los datos están autocorrelacionados positivamente, y si dicho valor es mayor a 2 pero menor que 4 se dice que hay una autocorrelación negativa. Sin embargo, si el valor es de 2 puede concluirse que existe 0 autocorrelación entre las variables del dataset. Ahora bien, de lo anterior sólo puede concluirse dependencia en los dos primeros casos mientras que en el caso de probar una autocorrelación de 0 sólo podemos sugerir que hay evidencia necesaria para suponer independencia pero no suficiente.\n",
        "\n",
        "Otra prueba que evalúa el nivel de independencia entre dos variables es la **prueba de bondad de ajuste de Chi-cuadrado**. Dicha prueba no evalúa la independencia de las observaciones en un dataset, sin embargo dado dicho conjunto de datos prueba si las variables evaluadas (que suelen ser conteos de variables categóricas, ej. Matriz de confusión), en las filas son independientes de las variables en las columnas (en el caso de un arreglo tabular bidimensional). La hipótesis nula de dicha prueba es que las variables son independientes, sin embargo, si se rechaza la hipótesis nula puede sugerirse que existe una dependencia entre las variables y por lo tanto una correlación aparente entre las mismas.\n",
        "\n",
        "Para una discusión más amplia de este problema pueden consultar el siguiente trabajo:\n",
        "https://link.springer.com/chapter/10.1007/978-1-4612-3794-5_4"
      ]
    }
  ]
}